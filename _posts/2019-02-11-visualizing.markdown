---
layout: post
title:      "Visualizing"
date:       2019-02-11 10:33:06 -0500
permalink:  visualizing
---


This blog post will explore the visual aspect of exploratory data analysis using King's County Housing Prices dataset.

A big part of exploratorty data analysis is visualisation. Thousands of rows of data can expresssed simultaneously as points, lines, bars, areas and many other forms and this is very useful for identifying and exploring trends or important aspects of the data set. Once the data set has been cleaned of missing values, one of the first steps of exploratory data analysis that we have been shown is to generate histograms of each feature. Our visualisations so far have been generated by matplotlib.pyplot and some have used seaborn. Generating histograms for each feature of a dataset individually would be very repetitive, a for loop to repeat the steps automatically is great shortcut but even better when using pandas this can be done simply with the command df.hist()

![initial histograms](https://i.imgur.com/81SfMKV.png)

This is perfect, one short line of code for 18 histograms. The first step in visualizing our data and now we can make some decisions about what to do with it. The end goal of our project is an Ordinary Least Squares Regression model for the target feature price and the regression method we use has someassumptions, the first of which is normality. Does the data follow a normal distribution? The histograms we just generated are a great visual test for normality and skew. From this visualisation we can make transformations on our feature set to push the data into the shape that we want for the regression. Price is log transformed and so is sqft_living and new histograms can be generated to show off this distribution.

![log transformed histograms](https://i.imgur.com/YOH5OiM.png)
![log transformed histograms](https://i.imgur.com/B8yupCh.png)

Much better fit for normal distribution and we can also plot the two features against each other for a visual representation of their relationship

![scatter](https://i.imgur.com/PjukAOM.png)

This scatter plot is generated using seaborn rather than matplotlib to take advantage of slight differences in their default settings. Seaborn employs white rings around blue circles as its default setting and matplotlib plots plain blue circles. The difference here is that when data is dense and overlapping the matplotlib version creates a solid blue mass whereas in the seaborn version each data point is still recognisable. There are other ways to make overlapping data more visible that could be employed. The 'alpha' setting when plotting allows for transparency of graphed features so overlapping datapoints can be seen as the overlap creates a more noticable colour. This was utilised in another scatter plot of this data where color coding was used to make a 2D scatter plot representative of a 3rd freature, effectively adding a 3rd dimension.

![jones](https://i.imgur.com/j4VBYZT.png)

This is the same plot! Using matplotlib this time the same scatter plot was generated and the third feature/dimension, the 'jones' feature, was added to the colour determine colour of the data points. For this visualisation I was only concerned with whether the jones factor was positive or negative so the code for generating this plot reads 

plt.scatter(log_sqft_living,price_log, alpha=0.6, c=jones>0)

c being the colour scale mapping function. By passing the determination of jones>0 rather than just the jones value itself I cut out the spectrum of colours to a binary selection of two colours: purple or yellow. And by passing an aplha value of 0.6, where these data points overlap is more visible especially in the region between the two red lines where there are a mix of both purple and yellow dots. The colour bar is added to the plot by passing the commad plt.colorbar() and it allows for a full spectrum of color differences for a more nuanced 3D understandung of the data but that was not desirable in this case and a full colour spectrum as a plot scale can be more confusing than enlightening when not used carefully. As a part of the determination of factors usd in the regression model a heatmap of the correlation factor of each feature was generated.


![heatmap](https://i.imgur.com/4mFHCp0.png)

sns.heatmap(np.abs(corr),linewidths=2,mask=mask,cmap=cmap,annot=True)

When generating this heatmap I was conscious that usinig colour as a scale can be difficult. I've tried to add as much clarity as possible, the boxes are spaced out with a linewidth divider and the colour scale represents the absolute value of the correlation but still I felt it was necessary to annotate the boxes with their values. Taking the absolute value makes the data much more comparable by colour 0.8 and -0.79 are almost equally far from zero but if these were to be represented as red and green it would be impossible to tell which 'is this more green than this is red?'. In the end though it was important to make a cutoff call when choosing features and if I want this graph to tell me what I want to know (which features are above my cutoff) quickly than there is a more simple way to do it.

![heatmap2](https://i.imgur.com/ZzXrw67.png)

Here it is, black and white. Again I have used a determination in the colour field, this time whether the absolute  vale correlation factor was above 0.5

sns.heatmap(abs(corr)>.5,mask=mask,linewidths=2)

This creates an easy at-a-glance version of the heatmap. It tells us less but it tells us what we need to know and nothing else. And that just might be the perfect kind of visualisation


